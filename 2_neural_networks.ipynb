{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll also need numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the MNIST dataset. The most famous datasets are available in the `torchvision` library, see an example on how to load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "train = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "train.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Labels\n",
    "train.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torchvision.datasets.MNIST('./', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "test.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Labels\n",
    "test.targets.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network topology\n",
    "\n",
    "### 1.1. Layers\n",
    "\n",
    "Linear dense layers (as well as most common layers) are available in `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_linear = nn.Linear(784, 500)\n",
    "second_linear = nn.Linear(500,200)\n",
    "third_linear = nn.Linear(200,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d (input_channels, output_channels, stride)\n",
    "first_conv = nn.Conv2d(1, 5, (5,5))\n",
    "second_conv = nn.Conv2d(5, 10, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (input_size, hidden_size, num_layers)\n",
    "first_rnn = nn.LSTM(10, 50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Normalization and Regularization Layers\n",
    "The most common normalization and regularization strategies are also implemented by default in `torch.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout(p=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Activation functions\n",
    "A class version of the functions is available under `torch.nn`. The function versions are under `torch.nn.functional`, here are the most commonly used ones\n",
    "\n",
    "#### Rectifier Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.leaky_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network modules\n",
    "### 2.1. Sequential module\n",
    "Sequential modules are built by passing a sequence of layers and activators to the `nn.Sequential` constructor. The layers and functions will be applied in the provided order, hence the dimensions of consecutive layers must match each other.\n",
    "\n",
    "Let's build a simple autoencoder with 4 layers and 2 latent dimensions:\n",
    "\n",
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(nn.Linear(28*28, 128), nn.ReLU(),\\\n",
    "                        nn.Linear(128, 64), nn.ReLU(),\\\n",
    "                        nn.Linear(64, 12), nn.ReLU(),\\\n",
    "                        nn.Linear(12, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = nn.Sequential(nn.Linear(2, 12), nn.ReLU(),\\\n",
    "                        nn.Linear(12, 64), nn.ReLU(),\\\n",
    "                        nn.Linear(64, 128), nn.ReLU(),\\\n",
    "                        nn.Linear(128, 28*28), nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = nn.Sequential(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Module Class\n",
    "More complex modules and non-sequential topologies must be implemented with custom classes inheriting `torch.nn.Module`. The classes must define the method `forward()` which describes how to generate the output given the input data.\n",
    "\n",
    "Let's construct the same autoencoder Module classes:\n",
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, lat_dim):\n",
    "        # Call superClass initializer\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(28*28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_3 = nn.Linear(64, 12)\n",
    "        self.layer_4 = nn.Linear(12, lat_dim)\n",
    "    \n",
    "    def forward(self, data_in):\n",
    "        # Compute the output\n",
    "        h = func.relu(self.layer_1(data_in))\n",
    "        h = func.relu(self.layer_2(h))\n",
    "        h = func.relu(self.layer_3(h))\n",
    "        z = self.layer_4(h)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, lat_dim):\n",
    "        # Call superClass initializer\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(lat_dim, 12)\n",
    "        self.layer_2 = nn.Linear(12, 64)\n",
    "        self.layer_3 = nn.Linear(64, 128)\n",
    "        self.layer_4 = nn.Linear(128, 28*28)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Compute the output\n",
    "        h = func.relu(self.layer_1(z))\n",
    "        h = func.relu(self.layer_2(h))\n",
    "        h = func.relu(self.layer_3(h))\n",
    "        data_out = torch.sigmoid(self.layer_4(h))\n",
    "        \n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, lat_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(lat_dim)\n",
    "        self.decoder = Decoder(lat_dim)\n",
    "        \n",
    "    def forward(self, data_in):\n",
    "        lat = self.encoder(data_in)\n",
    "        data_out = self.decoder(lat)\n",
    "        \n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(autoencoder.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "### 3.1. Optimizer\n",
    "To train a network we need to instantiate a _parameter optimizer_ and specify the parameters to be optimized. The parameter optimizer provides a `step()` function which will update the parameters using the gradients stored in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Learning Rates\n",
    "Different learning rates can be specified for a specific set of parameters. For instance, if we want to train the decoder faster, we could define the optimizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven_optimizer = torch.optim.Adam([{'params': encoder.parameters(), 'lr': 1e-3}, {'params': decoder.parameters(), 'lr': 5e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uneven_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also train at different learning rates per layer passing a dictionary with `first_layer.parameters()`, and so on.\n",
    "\n",
    "#### Multiple Optimizers\n",
    "Note that many optimizers may be instantiated simultaneously, since they don't interfere with each other as long as each gradient step is performed only with one optimizer. That means we could train the first N steps with `uneven_optimizer.step()` and then continue with even learning rates using `optimizer.step()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loss function\n",
    "The loss function is the metric to minimize during training. It is usually defined as a function of the input and the output data. Loss functions are defined in `torch.nn.functional`, but we can always define them manually by applying arithmetic operations to the data.\n",
    "\n",
    "The goal of an autoencoder is to produce outputs that resemble the inputs, so we will use the binary crossentropy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Mini-batch training\n",
    "Once an optimizer and a Loss function have been chosen, we can proceed to train our network with mini-batches.\n",
    "\n",
    "We will compute the gradients of the parameters with batches of **100 training samples** during **10 epochs**. \n",
    "\n",
    ">Note that the gradients of the parameters are computed and stored in their containing tensor when we call `backward()`. The `optimizer.step()` function is called immediately after to update the parameters and the stored gradients are reset using `optimizer.zero_grad()`. If `zero_grad()` is not called after each step, we would keep accumulating the gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "n_test_img = 4\n",
    "epochs     = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Reshape data\n",
    "train_samples = train.data.view(-1, 28*28).type(torch.float32)/255.0\n",
    "test_samples  = test.data.view(-1,28*28).type(torch.float32)/255.0\n",
    "\n",
    "# Lists to store training losses\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "\n",
    "# Plot test input images\n",
    "test_imgs = test.data[0:n_test_img,:].type(torch.float32).view(-1,28*28)/255.0\n",
    "f, a = plt.subplots(2, n_test_img, figsize=(8, 3))\n",
    "for i in range(n_test_img):\n",
    "    a[0][i].imshow(255-np.reshape(test_imgs.data.numpy()[i], (28,28)), cmap='gray')\n",
    "    a[0][i].set_xticks(())\n",
    "    a[0][i].set_yticks(())\n",
    "    \n",
    "loss_text = f.text(0, 0, \"epoch: 0, loss: 0\")\n",
    "\n",
    "# Data iterator\n",
    "train_batches = Data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for e in np.arange(epochs):\n",
    "    batch_loss = 0\n",
    "    for batch_no, (batch, batch_labels) in enumerate(train_batches):\n",
    "        # Input and target data (flatten)\n",
    "        b_in = batch.view(-1, 28*28)\n",
    "        target = batch.view(-1, 28*28)\n",
    "        # Forward pass of the data through the network\n",
    "        out  = autoencoder(b_in)\n",
    "        # Compute the Loss\n",
    "        loss = func.binary_cross_entropy(out, target)\n",
    "        batch_loss += loss\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Test images\n",
    "        if batch_no % 50 == 0:\n",
    "            test_out = autoencoder(test_imgs)\n",
    "            for i in range(n_test_img):\n",
    "                a[1][i].imshow(1.0-np.reshape(test_out.data.numpy()[i], (28,28)), cmap='gray')\n",
    "                a[1][i].set_xticks(())\n",
    "                a[1][i].set_yticks(())\n",
    "            loss_text.set_text(\"epoch: {}, loss: {:.3f}\".format(e+1, loss))\n",
    "            f.canvas.draw()\n",
    "            \n",
    "    # Compute batch loss\n",
    "    train_loss.append(batch_loss/batch_no)\n",
    "    test_loss.append(func.binary_cross_entropy(autoencoder(test_samples), test_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.legend([\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "### 4.1. Loss on test dataset\n",
    "We can easily compute the Loss by evaluating the loss function on the output data:\n",
    "\n",
    "#### Train Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loss: {}\".format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {}\".format(test_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Latent space\n",
    "The projections of the test images in the latent space after training look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lat = autoencoder.encoder(test_samples).data.numpy()\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(test_lat[:,0], test_lat[:,1], c=test.targets.numpy(), s=1.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
