{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Uncertainty Autoencoder\n",
    "_(Requires Python 3, PyTorch 1.0.1, TorchVision 0.2.2)_\n",
    "\n",
    "**Reference**: _C. Blundell et al,_ [Weight Uncertainty in Neural Networks](https://arxiv.org/abs/1505.05424)\n",
    "\n",
    "### Libraries\n",
    "Import torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.utils.data as Data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CUDA configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "assert torch.cuda.current_device() == 0 \n",
    "assert torch.cuda.device_count() >= 1\n",
    "assert torch.cuda.memory_allocated() == 0\n",
    "assert torch.cuda.memory_cached() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Weight Uncertainty Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WULayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, prev_d, d):\n",
    "        super(WULayer, self).__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(d, prev_d))\n",
    "        # Initialize 'ro' so that sampled weights are small (sd on output is 0.05).\n",
    "        self.ro = nn.Parameter(torch.log(torch.exp(1.5*torch.ones(d, prev_d)/np.sqrt(d))-1))\n",
    "        self.bs = nn.Parameter(torch.zeros(d, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sigma = torch.log(1 + torch.exp(self.ro))\n",
    "        weights = torch.randn(self.mu.shape).cuda() * sigma + self.mu\n",
    "        loss_postr = torch.sum(Normal(self.mu,sigma).log_prob(weights))\n",
    "        mix_a = Normal(0,0.01).log_prob(weights) + torch.log(torch.tensor([0.7]).cuda())\n",
    "        mix_b = Normal(0,2).log_prob(weights)   + torch.log(torch.tensor([0.3]).cuda())\n",
    "        loss_prior = torch.sum(torch.logsumexp(torch.cat([mix_a.view(-1,1), mix_b.view(-1,1)], dim=1),dim=1))\n",
    "        return torch.mm(weights, x) + self.bs, loss_postr - loss_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder(nn.Module):\n",
    "\n",
    "    def __init__(self, i_dim, h_dim, o_dim):\n",
    "        super(Coder, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.i_dim  = i_dim\n",
    "        self.h_dims = np.array(h_dim)\n",
    "        self.o_dim  = o_dim\n",
    "\n",
    "        prev_d = i_dim\n",
    "        for d in h_dim:\n",
    "            self.layers.append(WULayer(prev_d, d))\n",
    "            prev_d = d\n",
    "        self.layers.append(WULayer(prev_d, o_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        loss = 0\n",
    "        for layer in self.layers[:-1]:\n",
    "            x, l = layer(x)\n",
    "            x = func.relu(x)\n",
    "            loss += l\n",
    "        x, l = self.layers[-1](x)\n",
    "        loss += l\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Uncertainty AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WUAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, i_dim, h_dim, l_dim):\n",
    "        super(WUAE, self).__init__()\n",
    "        self.encoder = Coder(i_dim, h_dim, l_dim)\n",
    "        self.decoder = Coder(l_dim, np.flip(h_dim), i_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.t(x)\n",
    "        z, le = self.encoder(x)\n",
    "        x, ld = self.decoder(z)\n",
    "        return torch.t(torch.sigmoid(x)), le+ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.loadtxt('sc_mouse_binary.txt.gz', delimiter='\\t', dtype=int)\n",
    "# Generate random indices\n",
    "np.random.seed(10)\n",
    "idx = np.arange(len(all_data))\n",
    "np.random.shuffle(idx)\n",
    "# Take 10% for test\n",
    "test_idx = idx[:len(all_data)/10,:]\n",
    "train_idx = idx[len(all_data)/10:,:]\n",
    "# Generate sets\n",
    "train_data = all_data[train_idx,:].cuda()\n",
    "test_data = all_data[test_idx,:].cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate WUAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dim = train.shape[1]\n",
    "h_dim = [1500, 800, 600]\n",
    "o_dim = 10\n",
    "\n",
    "wuae = WUAE(i_dim, h_dim, o_dim).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert next(wuae.parameters()).is_cuda\n",
    "assert next(wuae.encoder.parameters()).is_cuda\n",
    "assert next(wuae.decoder.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs   = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(wuae.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100], gamma=.316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Lists to store training losses\n",
    "train_rec_loss = []\n",
    "train_weight_loss = []\n",
    "test_rec_loss  = []\n",
    "\n",
    "# Set model to training mode\n",
    "wuae.train()\n",
    "\n",
    "loss_text = f.text(0, 0, \"Initializing...\")\n",
    "\n",
    "for e in np.arange(n_epochs):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(epoch_idx)\n",
    "    batch_idx = np.array_split(epoch_idx, train_data.shape[0]/batch_size)\n",
    "    N = len(batch_idx)\n",
    "\n",
    "    rec_loss = 0\n",
    "    weight_loss = 0\n",
    "    batch_loss = 0\n",
    "    for batch_no, idx in enumerate(batch_idx):\n",
    "        # Input and target data\n",
    "        x = train_data[idx,:]\n",
    "        #noisy_x = x.clone().detach().cuda()\n",
    "        #noise_idx = torch.FloatTensor(noisy_x.shape).uniform_(0,1) < .1\n",
    "        #noisy_x[noise_idx] = torch.FloatTensor(noisy_x.shape).uniform_(0,1)[noise_idx].cuda()\n",
    "        # Forward pass of the data through the network\n",
    "        y, w_loss = wuae(x)\n",
    "        # Compute the loss\n",
    "        pi_i = 2**(len(train_batches)-batch_no-1) / (2**len(train_batches)-1)\n",
    "        loss = nn.functional.binary_cross_entropy(y,x,reduction='sum')\n",
    "        rec_loss += float(loss) / len(train_samples)\n",
    "        weight_loss += pi_i * float(w_loss)\n",
    "        batch_loss += float(loss) + pi_i * float(w_loss)\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        del loss\n",
    "\n",
    "    # End of epoch, compute train & test loss\n",
    "    train_rec_loss.append(rec_loss)\n",
    "    train_weight_loss.append(weight_loss)\n",
    "    out, _ = wuae(test_data)\n",
    "    loss = float(nn.functional.binary_cross_entropy(out, test_data, reduction='sum')) / len(test_data)\n",
    "    test_rec_loss.append(float(loss))\n",
    "    \n",
    "    print(\"epoch: {}, train: {:.3f}, test: {:.3f}\".format(e+1, rec_loss, float(loss)))\n",
    "\n",
    "\n",
    "    scheduler.step(batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(train_rec_loss)\n",
    "plt.plot(test_rec_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wuae, 'wuae_bsc_wo_dropout.trc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
