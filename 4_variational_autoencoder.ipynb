{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "_(Requires Python 3, PyTorch 1.0.1, TorchVision 0.2.2)_\n",
    "\n",
    "**Reference**: _D.Kingma and M.Welling,_ [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "### Libraries\n",
    "Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll also need numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The variational autoencoder has a conventional structure of encoder plus decoder. \n",
    "\n",
    "In a VAE, the encoder does not directly generate the latent representations. Instead, it generates the parameters for a multidimensional random variable whose distribution is to be defined. \n",
    "\n",
    "> We will use independent multivariate Normal distributions, so our encoder will generate the mean and the standard deviation for each latent dimension.\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, lat_dim, activations=func.relu):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Parse input arguments\n",
    "        if type(hidden_dims) == int:\n",
    "            hidden_dims = [hidden_dims,]\n",
    "\n",
    "        if type(activations) == list:\n",
    "            if len(activations) != len(hidden_dims):\n",
    "                raise ValueError('activations and hidden_dims must have the same dimensions')\n",
    "        else:\n",
    "            activations = [activations]*len(hidden_dims)\n",
    "\n",
    "        # Store arguments\n",
    "        self.input_dim   = input_dim\n",
    "        self.lat_dim     = lat_dim\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_d = input_dim\n",
    "        for d in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_d, d))\n",
    "            prev_d = d\n",
    "            \n",
    "        # Latent layer parameters\n",
    "        self.mu = nn.Linear(prev_d, lat_dim)\n",
    "        self.logvar = nn.Linear(prev_d, lat_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x))\n",
    "            \n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        \n",
    "        z = mu + torch.exp(0.5*logvar)*torch.randn(mu.shape)\n",
    "        \n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, lat_dim, hidden_dims, output_dim, activations=func.relu):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Parse input arguments\n",
    "        if type(hidden_dims) == int:\n",
    "            hidden_dims = [hidden_dims,]\n",
    "        \n",
    "        if type(activations) == list:\n",
    "            if len(activations) != len(hidden_dims):\n",
    "                raise ValueError('activations and hidden_dims must have the same dimensions')\n",
    "        else:\n",
    "            activations = [activations]*len(hidden_dims)\n",
    "            \n",
    "        # Store arguments\n",
    "        self.input_dim   = input_dim\n",
    "        self.lat_dim     = lat_dim\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        activations.append(torch.sigmoid)\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_d = lat_dim\n",
    "        for d in self.hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_d, d))\n",
    "            prev_d = d\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(prev_d, output_dim))\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, intput_dim, hidden_dims, lat_dim, activations=func.relu):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Parse input arguments\n",
    "        if type(hidden_dims) == int:\n",
    "            hidden_dims = [hidden_dims,]\n",
    "        \n",
    "        if type(activations) == list:\n",
    "            if len(activations) != len(hidden_dims):\n",
    "                raise ValueError('activations and hidden_dims must have the same dimensions')\n",
    "        else:\n",
    "            activations = [activations]*len(hidden_dims)\n",
    "            \n",
    "        # Store arguments\n",
    "        self.input_dim   = input_dim\n",
    "        self.lat_dim     = lat_dim\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        \n",
    "        # Create encoder-decoder\n",
    "        self.encoder = Encoder(input_dim, hidden_dims, lat_dim)\n",
    "        self.decoder = Decoder(lat_dim, np.flip(hidden_dims, 0), input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        \n",
    "        return out, mu, logvar, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training example\n",
    "\n",
    "### Training Data\n",
    "Let's train the denoising autoencoder on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "train = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test = torchvision.datasets.MNIST('./', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = 28*28\n",
    "layers     = [200,100,30]\n",
    "latent_dim = 2\n",
    "\n",
    "vae = VariationalAutoencoder(input_dim, layers, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "n_test_img = 6\n",
    "epochs     = 15\n",
    "batch_size = 100\n",
    "\n",
    "# Reshape data\n",
    "train_samples = train.data.view(-1, 28*28).type(torch.float32)/255.0\n",
    "test_samples  = test.data.view(-1,28*28).type(torch.float32)/255.0\n",
    "\n",
    "# Lists to store training losses\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "\n",
    "# Set model to training mode\n",
    "vae.train()\n",
    "\n",
    "# Plot test input images\n",
    "test_imgs = test.data[0:n_test_img,:].type(torch.float32).view(-1,28*28)/255.0\n",
    "f, a = plt.subplots(2, n_test_img, figsize=(8, 3))\n",
    "for i in range(n_test_img):\n",
    "    a[0][i].imshow(255-np.reshape(test_imgs.data.numpy()[i], (28,28)), cmap='gray')\n",
    "    a[0][i].set_xticks(())\n",
    "    a[0][i].set_yticks(())\n",
    "    \n",
    "loss_text = f.text(0, 0, \"epoch: 0, loss: 0\")\n",
    "\n",
    "# Data iterator\n",
    "train_batches = Data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for e in np.arange(epochs):\n",
    "    batch_loss = 0\n",
    "    for batch_no, (batch, batch_labels) in enumerate(train_batches):\n",
    "        # Input and target data (flatten)\n",
    "        b_in = batch.view(-1, 28*28)\n",
    "        target = batch.view(-1, 28*28)\n",
    "        # Forward pass of the data through the network\n",
    "        out, mu, logvar, z = vae(b_in)\n",
    "        # Compute the VAE Losses\n",
    "        ae_loss = func.binary_cross_entropy(out, target, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Total loss \n",
    "        loss = ae_loss + kl_loss\n",
    "        batch_loss += float(loss)\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Test images\n",
    "        if batch_no % 50 == 0:\n",
    "            test_out, _, _, _ = vae(test_imgs)\n",
    "            for i in range(n_test_img):\n",
    "                a[1][i].imshow(1.0-np.reshape(test_out.data.numpy()[i], (28,28)), cmap='gray')\n",
    "                a[1][i].set_xticks(())\n",
    "                a[1][i].set_yticks(())\n",
    "            loss_text.set_text(\"epoch: {}, loss: {:.3f}\".format(e+1, loss))\n",
    "            f.canvas.draw()\n",
    "\n",
    "    # End of epoch, compute train & test loss\n",
    "    train_loss.append(batch_loss/batch_no)\n",
    "    out, mu, logvar, z = vae(test_samples)\n",
    "    test_loss.append(func.binary_cross_entropy(out, test_samples) - 0.5*torch.sum(1+logvar - mu.pow(2) - logvar.exp()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Loss on test dataset\n",
    "We can easily compute the Loss by evaluating the loss function on the output data:\n",
    "\n",
    "#### Train Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loss: {}\".format(train_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss: {}\".format(test_loss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space\n",
    "The projections of the test images in the latent space after training look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z, test_mu, test_logsigma = vae.encoder(test_samples)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(test_mu.data.numpy()[:,0], test_mu.data.numpy()[:,1], c=test.targets.numpy(), s=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_z, test_mu, test_logsigma = vae.encoder(train_samples)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(test_mu.data.numpy()[:,0], test_mu.data.numpy()[:,1], c=train.targets.numpy(), s=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "Generate images sampling the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = np.arange(2,-2.1,-0.2) # 21 samples from -2 to 2\n",
    "y_samples = np.arange(-2,2.1,0.2) # 21 samples from -2 to 2\n",
    "\n",
    "imgs = np.zeros((28*21, 28*21))\n",
    "\n",
    "for y_idx, y in enumerate(y_samples):\n",
    "    for x_idx, x in enumerate(x_samples):\n",
    "        y_img = 1-vae.decoder(torch.Tensor([x,y])).view(-1,28,28)\n",
    "        imgs[(x_idx*28):((x_idx+1)*28), (y_idx*28):((y_idx+1)*28)] = y_img.detach().numpy()\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(imgs, cmap=plt.cm.gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
