{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semisupervised Adversarial Autoencoder\n",
    "_(Requires Python 3, PyTorch 1.0.1, TorchVision 0.2.2)_\n",
    "\n",
    "**Reference**: _A.Makhzani et al.,_ [Adversarial Autoencoders](https://arxiv.org/abs/1511.05644)\n",
    "\n",
    "### Libraries\n",
    "Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.utils.data as Data\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll also need numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The design is essentially the same as the unsupervised adversarial autoencoder, with only one difference: a subset of input samples will have label information!\n",
    "\n",
    "We will use this extra information to place different numbers in different distributions. For instance, to allocate the 10 digits we can construct a mixture of 10 Gaussian distributions.\n",
    "\n",
    "#### Sample information as latent space regularizer\n",
    "\n",
    "The prior for the latent space in this example will be a mixture of 10 Gaussian distributions, with standard deviation 1, and their means equally spaced on a circle of radius 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = 10*np.cos(np.arange(0,10) * 2*np.pi/10)\n",
    "y_mean = 10*np.sin(np.arange(0,10) * 2*np.pi/10)\n",
    "\n",
    "samples = np.zeros((10000,2))\n",
    "for digit in np.arange(10):\n",
    "        samples[digit*1000:(digit+1)*1000,:] = np.random.multivariate_normal(np.array([x_mean[digit], y_mean[digit]]), np.eye(2), size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[:,0], samples[:,1], c=np.repeat(np.arange(10), 1000), s=0.7)\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label information will be used as a _mixture switch_, _i.e._ to pick one of the distributions from the mixture model, thence forcing the encoder to place the digit in that region of the latent space.\n",
    "\n",
    "For instance, when the input sample has label `0`, the following distribution will be passed to the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[:1000,0], samples[:1000,1], c=np.repeat(np.arange(10), 1000)[:1000], s=0.7)\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the same for the digit `7`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[6000:7000,0], samples[6000:7000,1], c=np.repeat(np.arange(10), 1000)[6000:7000], s=0.7)\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens when the input sample does not have label information? We will let the encoder choose. But only within our mixture limits! In effect, we will present the full mixture to the discriminator. We expect that the unlabeled ones will be eventually placed close to their class mixture.\n",
    "\n",
    "So, when the input sample is of `unknown` class, the presented prior will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples[:,0], samples[:,1], c='gray', s=0.7)\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(hidden_dims, activations):\n",
    "    # Parse input arguments\n",
    "    if type(hidden_dims) == int:\n",
    "        hidden_dims = [hidden_dims,]\n",
    "\n",
    "    if type(activations) == list:\n",
    "        if len(activations) != len(hidden_dims):\n",
    "            raise ValueError('activations and hidden_dims must have the same dimensions')\n",
    "    else:\n",
    "        activations = [activations]*len(hidden_dims)\n",
    "\n",
    "    return hidden_dims, activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, activations=func.relu, dropout=False, dropout_p=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Latent dimensions is hard-coded in this example\n",
    "        lat_dim = 2\n",
    "        \n",
    "        # Parse input arguments\n",
    "        hidden_dims, activations = parse_args(hidden_dims, activations)\n",
    "\n",
    "        # Store arguments\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        self.dropout     = dropout\n",
    "        self.dropout_p   = dropout_p\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_d = input_dim\n",
    "        for d in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_d, d))\n",
    "            prev_d = d\n",
    "            \n",
    "        # Latent layer\n",
    "        self.out_layer = nn.Linear(prev_d, lat_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x)) if not self.dropout else activation(func.dropout(layer(x), training=self.training, p=self.dropout_p))        \n",
    "        \n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dims, output_dim, activations=func.relu, dropout=False, dropout_p=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Latent dimensions is hard-coded in this example\n",
    "        lat_dim = 2\n",
    "        \n",
    "        # Parse input arguments\n",
    "        hidden_dims, activations = parse_args(hidden_dims, activations)\n",
    "            \n",
    "        # Store arguments\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        self.dropout     = dropout\n",
    "        self.dropout_p   = dropout_p\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_d = lat_dim\n",
    "        for d in self.hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_d, d))\n",
    "            prev_d = d\n",
    "            \n",
    "        # Output layer\n",
    "        self.out_layer = nn.Linear(prev_d, output_dim)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x)) if not self.dropout else activation(func.dropout(layer(x), training=self.training, p=self.dropout_p))\n",
    "        \n",
    "        return torch.sigmoid(self.out_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dims, class_count, activations=func.relu, dropout=False, dropout_p=0.3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Latent dimensions is hard-coded in this example\n",
    "        lat_dim = 2\n",
    "        \n",
    "        # Parse input arguments\n",
    "        hidden_dims, activations = parse_args(hidden_dims, activations)\n",
    "        \n",
    "        # Store arguments\n",
    "        self.hidden_dims = np.array(hidden_dims)\n",
    "        self.activations = activations\n",
    "        self.dropout     = dropout\n",
    "        self.dropout_p   = dropout_p\n",
    "        self.class_count = class_count\n",
    "        \n",
    "        # One-hot representation\n",
    "        self.to_one_hot = torch.eye(self.class_count+1)\n",
    "\n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_d = lat_dim + class_count + 1\n",
    "        for d in self.hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_d, d))\n",
    "            prev_d = d\n",
    "            \n",
    "        # Output layer\n",
    "        self.out_layer = nn.Linear(prev_d, 1)\n",
    "    \n",
    "    def forward(self, z, l_int):\n",
    "        # Receives sample from latent space (z) and label information (l)\n",
    "        x = torch.cat((z, self.to_one_hot[l_int]), dim=1)\n",
    "        \n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = activation(layer(x)) if not self.dropout else activation(func.dropout(layer(x), training=self.training, p=self.dropout_p))\n",
    "        \n",
    "        return torch.sigmoid(self.out_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semisupervised Adversarial Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSAAE(nn.Module):\n",
    "    def __init__(self, encoder_module, decoder_module, discrim_module):\n",
    "        super(SSAAE, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.class_count = discrim.class_count\n",
    "        \n",
    "        # AAE modules\n",
    "        self.encoder = encoder_module\n",
    "        self.decoder = decoder_module\n",
    "        self.discrim = discrim_module\n",
    "        \n",
    "        # Prior distribution\n",
    "        x_mean = self.class_count * np.cos(np.arange(0,self.class_count) * 2*np.pi/self.class_count).reshape((-1,1))\n",
    "        y_mean = self.class_count * np.sin(np.arange(0,self.class_count) * 2*np.pi/self.class_count).reshape((-1,1))\n",
    "        self.p_mean = np.concatenate((x_mean, y_mean), axis=1)\n",
    "        \n",
    "        # Number mapping (place them by similarity)\n",
    "        digit_order = [0,5,9,4,8,2,1,6,3,7]\n",
    "        self.p_mean = self.p_mean[digit_order]\n",
    "        \n",
    "    # This method generates samples for the desired prior (based on sample label)\n",
    "    def p_samples(self, l):\n",
    "        size = l.shape[0]\n",
    "        \n",
    "        # Find unknowns\n",
    "        mean_id = l.detach().numpy()\n",
    "        no_l = np.where(mean_id == self.class_count)[0]\n",
    "        # Replace unknowns by random sample of means\n",
    "        mean_id[no_l] = np.random.randint(0,self.class_count,len(no_l))\n",
    "        # Generate gaussian mixtures\n",
    "        s = self.p_mean[mean_id,:] + np.random.randn(size, 2)\n",
    "            \n",
    "        return torch.Tensor(s)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        # Encoder\n",
    "        z = self.encoder(x)\n",
    "        # Decoder\n",
    "        y = self.decoder(z)\n",
    "        # Discriminator\n",
    "        d = self.discrim(z, l) if l is not None else 0\n",
    "        \n",
    "        return y, d, z        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training example\n",
    "\n",
    "### Training Data\n",
    "Let's train the denoising autoencoder on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "train = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test = torchvision.datasets.MNIST('./', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Autoencoder\n",
    "Again, we will use the same network presented in the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = 28*28\n",
    "class_count = 10 # 10 digits (MNIST)\n",
    "enc_layers = [1000,1000]\n",
    "dec_layers = [1000,1000]\n",
    "dis_layers = [1000,1000]\n",
    "\n",
    "encoder = Encoder(input_dim, enc_layers, dropout=False)\n",
    "decoder = Decoder(dec_layers, input_dim, dropout=False)\n",
    "discrim = Discriminator(dis_layers, class_count, dropout=False)\n",
    "\n",
    "ssaae = SSAAE(encoder, decoder, discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssaae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "We will follow the same logic as described in episode 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(ssaae.encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = torch.optim.Adam(ssaae.decoder.parameters(), lr=0.001)\n",
    "discrim_optimizer = torch.optim.Adam(ssaae.discrim.parameters(), lr=0.001, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "num_labels = 30000\n",
    "n_test_img = 6\n",
    "epochs     = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Reshape data\n",
    "train_samples = train.data.view(-1, 28*28).type(torch.float32)/255.0\n",
    "test_samples  = test.data.view(-1,28*28).type(torch.float32)/255.0\n",
    "n_train = train_samples.shape[0]\n",
    "n_test  = test_samples.shape[0]\n",
    "\n",
    "# Allow some labels for SemiSupervised Training\n",
    "num_unknown = n_train - num_labels\n",
    "train_targets_copy = train.targets.clone()\n",
    "train.targets[np.random.choice(n_train, replace=False, size=num_unknown)] = 10\n",
    "\n",
    "# Lists to store training losses\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "\n",
    "# Plot test input images\n",
    "test_imgs = test.data[0:n_test_img,:].type(torch.float32).view(-1,28*28)/255.0\n",
    "f, a = plt.subplots(2, n_test_img, figsize=(8, 3))\n",
    "for i in range(n_test_img):\n",
    "    a[0][i].imshow(255-np.reshape(test_imgs.data.numpy()[i], (28,28)), cmap='gray')\n",
    "    a[0][i].set_xticks(())\n",
    "    a[0][i].set_yticks(())\n",
    "    \n",
    "loss_text = f.text(0, 0.01, \"epoch: 0, loss: 0\")\n",
    "\n",
    "# Discriminator targets\n",
    "target_real = torch.ones(batch_size,1)\n",
    "target_fake = torch.zeros(batch_size,1)\n",
    "target_test_real = torch.ones(n_test,1)\n",
    "target_test_fake = torch.ones(n_test,1)\n",
    "\n",
    "# Data iterator\n",
    "train_batches = Data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Loss records\n",
    "aenc_train_loss = np.zeros(epochs)\n",
    "disc_train_loss = np.zeros(epochs)\n",
    "disc_train_acc  = np.zeros(epochs)\n",
    "encd_train_loss = np.zeros(epochs)\n",
    "\n",
    "aenc_test_loss = np.zeros(epochs)\n",
    "disc_test_loss = np.zeros(epochs)\n",
    "disc_test_acc  = np.zeros(epochs)\n",
    "encd_test_loss = np.zeros(epochs)\n",
    "\n",
    "# Learning rate Schedulers\n",
    "encoder_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=10)\n",
    "decoder_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=10)\n",
    "discrim_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(discrim_optimizer, 'min', patience=10)\n",
    "\n",
    "for e in np.arange(epochs):\n",
    "    # Batch Loss\n",
    "    aenc_bloss = 0\n",
    "    disc_bloss = 0\n",
    "    encd_bloss = 0\n",
    "    \n",
    "    # Batch Accuracy\n",
    "    disc_bacc = 0\n",
    "    \n",
    "    for batch_no, (batch, batch_label) in enumerate(train_batches):\n",
    "        # Input and target data (flatten)\n",
    "        batch_input = batch.view(-1, 28*28)\n",
    "        \n",
    "        ## Train discriminator ##\n",
    "        # Samples\n",
    "        ssaae.eval()\n",
    "        z_fake = ssaae.encoder(batch_input)\n",
    "        z_real = ssaae.p_samples(batch_label)\n",
    "        \n",
    "        # Forward data\n",
    "        ssaae.train()\n",
    "        d = ssaae.discrim(torch.cat((z_fake.detach(), z_real), 0), torch.cat((batch_label, batch_label), 0))\n",
    "        \n",
    "        # Discriminator loss\n",
    "        disc_loss = func.binary_cross_entropy(d, torch.cat((target_fake, target_real),0))\n",
    "        disc_bloss += disc_loss\n",
    "        disc_bacc += torch.sum((d[:batch_size] < 0.5) + (d[batch_size:] > 0.5)).data.numpy() / (2*batch_size)\n",
    "        \n",
    "        # Compute gradients\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        discrim_optimizer.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        \n",
    "        # Update discriminator parameters\n",
    "        discrim_optimizer.step()\n",
    "        \n",
    "        \n",
    "        ## Train autoencoder ##\n",
    "        # Forward pass of the data through the network\n",
    "        y, d, z = ssaae(batch_input, batch_label)\n",
    "        # Autoencoder loss\n",
    "        aenc_loss = func.binary_cross_entropy(y, batch_input)\n",
    "        fool_loss = func.binary_cross_entropy(d, target_real)\n",
    "        encd_loss = 0.99*aenc_loss + 0.01*fool_loss\n",
    "        #aenc_loss = func.mse_loss(y, batch_input)\n",
    "        aenc_bloss += aenc_loss\n",
    "        encd_bloss += encd_loss\n",
    "        # Compute gradients\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        discrim_optimizer.zero_grad()\n",
    "        encd_loss.backward()\n",
    "        # Update encoder-decoder parameters\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        # Test images\n",
    "        if batch_no % 50 == 0:\n",
    "            ssaae.eval()\n",
    "            test_out, _, _ = ssaae(test_imgs, None)\n",
    "            for i in range(n_test_img):\n",
    "                a[1][i].imshow(1.0-np.reshape(test_out.data.numpy()[i], (28,28)), cmap='gray')\n",
    "                a[1][i].set_xticks(())\n",
    "                a[1][i].set_yticks(())\n",
    "            #a[1][-1].clear()\n",
    "            #a[1][-1].scatter(z.data.numpy()[:,0], z.data.numpy()[:,1], c=batch_labels, s=0.5)\n",
    "            loss_text.set_text(\"epoch: {}, reconstruct: {:.3f}, encoder: {:.3f}, discrim: {:.3f} ({:.2f}%)\".format(\n",
    "                e+1,\n",
    "                aenc_bloss/(batch_no+1),\n",
    "                encd_bloss/(batch_no+1),\n",
    "                disc_bloss/(batch_no+1),\n",
    "                disc_bacc/(batch_no+1)*100))\n",
    "            f.canvas.draw()\n",
    "\n",
    "    ## End of epoch\n",
    "    # Train loss\n",
    "    aenc_train_loss[e] = aenc_bloss/batch_no\n",
    "    disc_train_loss[e] = disc_bloss/batch_no\n",
    "    encd_train_loss[e] = encd_bloss/batch_no\n",
    "    disc_train_acc[e] = disc_bacc/batch_no\n",
    "    # Decrease encoder dist LR\n",
    "    encoder_sched.step(encd_train_loss[e])\n",
    "    decoder_sched.step(aenc_train_loss[e])\n",
    "    discrim_sched.step(disc_train_acc[e])\n",
    "    # Test loss\n",
    "    # autoencoder loss\n",
    "    ssaae.eval()\n",
    "    y, d, z_fake = ssaae(test_samples, test.targets)\n",
    "    aenc_test_loss[e] = func.binary_cross_entropy(y, test_samples)\n",
    "    # fool loss\n",
    "    encd_test_loss[e] = 0.99*aenc_test_loss[e] + 0.01*func.binary_cross_entropy(d, target_test_real)\n",
    "    # discriminator loss\n",
    "    z_real = ssaae.p_samples(test.targets)\n",
    "    d = ssaae.discrim(torch.cat((z_fake.detach(), z_real), 0), torch.cat((test.targets, test.targets),0))\n",
    "    disc_test_loss[e] = func.binary_cross_entropy(d, torch.cat((target_test_fake, target_test_real),0))\n",
    "    disc_test_acc[e]  = torch.sum((d[:n_test] < 0.5) + (d[n_test:] > 0.5)).data.numpy() / (2*n_test)\n",
    "    \n",
    "train.targets = train_targets_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Loss plots\n",
    "#### Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(aenc_train_loss)\n",
    "plt.plot(aenc_test_loss)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(disc_train_loss)\n",
    "plt.plot(disc_test_loss)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(disc_train_acc)\n",
    "plt.plot(disc_test_acc)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Distribution Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(encd_train_loss)\n",
    "plt.plot(encd_test_loss)\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space\n",
    "The projections of the test images in the latent space after training look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = ssaae.encoder(test_samples)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(test_d.data.numpy()[:,0], test_d.data.numpy()[:,1], c=test.targets.numpy(), s=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = ssaae.encoder(train_samples)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(test_d.data.numpy()[:,0], test_d.data.numpy()[:,1], c=train.targets.numpy(), s=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "Generate images sampling the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = np.arange(10,-10.1,-0.5) # 41 samples from -2 to 2\n",
    "y_samples = np.arange(-10,10.1,0.5) # 41 samples from -2 to 2\n",
    "\n",
    "imgs = np.zeros((28*41, 28*41))\n",
    "\n",
    "for y_idx, y in enumerate(y_samples):\n",
    "    for x_idx, x in enumerate(x_samples):\n",
    "        y_img = 1.0 - ssaae.decoder(torch.Tensor([x,y])).view(-1,28,28)\n",
    "        imgs[(x_idx*28):((x_idx+1)*28), (y_idx*28):((y_idx+1)*28)] = y_img.detach().numpy()\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(imgs, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
